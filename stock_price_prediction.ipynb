{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f670e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q xgboost scikit-learn boruta_shap boruta matplotlib seaborn tensorflow joblib\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBRegressor\n",
    "from boruta import BorutaPy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "#\n",
    "def load_data(path=None, target_col=\"target\", date_col=None):\n",
    "    \"\"\"\n",
    "    Load CSV data. If no path provided, creates a synthetic example.\n",
    "    Expects a tabular dataset with numeric features and one target column.\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        \n",
    "        rng = np.random.default_rng(RANDOM_SEED)\n",
    "        n = 5000\n",
    "        X = pd.DataFrame(rng.normal(size=(n, 20)), columns=[f\"f{i}\" for i in range(20)])\n",
    "        \n",
    "        y = 3*X[\"f1\"] - 2*X[\"f5\"] + 0.5*X[\"f10\"] + rng.normal(scale=0.5, size=n)\n",
    "        df = X.copy()\n",
    "        df[target_col] = y\n",
    "        return df\n",
    "    df = pd.read_csv(path, parse_dates=[date_col] if date_col else None)\n",
    "    return df\n",
    "\n",
    "\n",
    "DATA_PATH = None  \n",
    "TARGET_COL = \"target\"\n",
    "DATE_COL = None   \n",
    "\n",
    "df = load_data(DATA_PATH, target_col=TARGET_COL, date_col=DATE_COL)\n",
    "print(\"data shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "\n",
    "def prepare_X_y(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    X = X.select_dtypes(include=[np.number]).copy()\n",
    "    y = df[target_col].values\n",
    "    return X, y\n",
    "\n",
    "X, y = prepare_X_y(df, TARGET_COL)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X_scaled, y, np.arange(len(y)), test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "\n",
    "joblib.dump(scaler, \"scaler.joblib\")\n",
    "\n",
    "# Cell 5: Hybrid Feature Selection (HFS) combining RFE + Boruta\n",
    "def hybrid_feature_selection(X_train, y_train, feature_names,\n",
    "                             rfe_estimator=None, n_features_to_select=None,\n",
    "                             boruta_estimator=None, boruta_iter=100):\n",
    "    \"\"\"\n",
    "    Returns selected feature names by intersection of RFE and Boruta.\n",
    "    Optionally switch to union by changing the return logic.\n",
    "    \"\"\"\n",
    "    # RFE with XGBoost\n",
    "    if rfe_estimator is None:\n",
    "        rfe_estimator = XGBRegressor(n_estimators=200, random_state=RANDOM_SEED, verbosity=0)\n",
    "    if n_features_to_select is None:\n",
    "        # heuristic: keep sqrt of features or 10 whichever smaller/larger based on context\n",
    "        n_features_to_select = max(5, int(np.sqrt(len(feature_names))*2))\n",
    "    rfe = RFE(estimator=rfe_estimator, n_features_to_select=n_features_to_select, step=0.1)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_support = np.array(feature_names)[rfe.support_]\n",
    "\n",
    "    # Boruta with RandomForest\n",
    "    if boruta_estimator is None:\n",
    "        boruta_estimator = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED, n_estimators=500)\n",
    "    boruta = BorutaPy(estimator=boruta_estimator, n_estimators='auto', verbose=0, random_state=RANDOM_SEED, max_iter=boruta_iter)\n",
    "    boruta.fit(X_train, y_train)\n",
    "    boruta_support = np.array(feature_names)[boruta.support_]\n",
    "\n",
    "    # HFS: intersection (only features both methods agree on)\n",
    "    selected_intersection = sorted(set(rfe_support).intersection(set(boruta_support)))\n",
    "    # Optionally union:\n",
    "    selected_union = sorted(set(rfe_support).union(set(boruta_support)))\n",
    "\n",
    "    return {\n",
    "        \"rfe\": sorted(list(rfe_support)),\n",
    "        \"boruta\": sorted(list(boruta_support)),\n",
    "        \"intersection\": selected_intersection,\n",
    "        \"union\": selected_union\n",
    "    }\n",
    "\n",
    "feature_names = list(X.columns)\n",
    "hfs = hybrid_feature_selection(X_train, y_train, feature_names, boruta_iter=50)\n",
    "print(\"RFE selected:\", hfs[\"rfe\"])\n",
    "print(\"Boruta selected:\", hfs[\"boruta\"])\n",
    "print(\"HFS intersection selected:\", hfs[\"intersection\"])\n",
    "print(\"HFS union selected:\", hfs[\"union\"])\n",
    "\n",
    "# Choose final set (use intersection for stricter selection; switch to \"union\" if desired)\n",
    "final_features = hfs[\"intersection\"] if len(hfs[\"intersection\"])>0 else hfs[\"union\"]\n",
    "print(\"Final features used for modeling:\", final_features)\n",
    "\n",
    "# Map back indices for train/test using selected features\n",
    "selected_idx = [feature_names.index(f) for f in final_features]\n",
    "X_train_sel = X_train[:, selected_idx]\n",
    "X_test_sel = X_test[:, selected_idx]\n",
    "\n",
    "# Cell 6: Train XGBoost on selected features\n",
    "xgb_model = XGBRegressor(n_estimators=500, learning_rate=0.05, random_state=RANDOM_SEED, verbosity=0)\n",
    "xgb_model.fit(X_train_sel, y_train, eval_set=[(X_test_sel, y_test)], early_stopping_rounds=20, verbose=False)\n",
    "joblib.dump(xgb_model, \"hfs_xgb_model.joblib\")\n",
    "\n",
    "# Cell 7: Build and train a Deep Neural Network for comparison\n",
    "def build_dnn(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "dnn = build_dnn(X_train_sel.shape[1])\n",
    "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "history = dnn.fit(X_train_sel, y_train, validation_data=(X_test_sel, y_test),\n",
    "                  epochs=200, batch_size=64, callbacks=[es], verbose=0)\n",
    "dnn.save(\"hfs_dnn_model.keras\")\n",
    "\n",
    "# Cell 8: Evaluation\n",
    "def evaluate_model(model, X_test, y_test, model_type=\"xgb\"):\n",
    "    if model_type == \"dnn\":\n",
    "        preds = model.predict(X_test).ravel()\n",
    "    else:\n",
    "        preds = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = math.sqrt(mse)\n",
    "    return {\"mse\": mse, \"mae\": mae, \"rmse\": rmse, \"preds\": preds}\n",
    "\n",
    "res_xgb = evaluate_model(xgb_model, X_test_sel, y_test, model_type=\"xgb\")\n",
    "res_dnn = evaluate_model(dnn, X_test_sel, y_test, model_type=\"dnn\")\n",
    "\n",
    "print(\"XGBoost (HFS) -> MSE: {:.5f}, MAE: {:.5f}, RMSE: {:.5f}\".format(res_xgb[\"mse\"], res_xgb[\"mae\"], res_xgb[\"rmse\"]))\n",
    "print(\"DNN    (HFS) -> MSE: {:.5f}, MAE: {:.5f}, RMSE: {:.5f}\".format(res_dnn[\"mse\"], res_dnn[\"mae\"], res_dnn[\"rmse\"]))\n",
    "\n",
    "# Cell 9: Quick plots\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(y_test[:200], label=\"true\")\n",
    "plt.plot(res_xgb[\"preds\"][:200], label=\"xgb_pred\")\n",
    "plt.plot(res_dnn[\"preds\"][:200], label=\"dnn_pred\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"Test predictions (first 200 samples)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
